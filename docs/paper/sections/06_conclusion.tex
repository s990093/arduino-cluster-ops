\section{Conclusion \& Future Work}

We successfully demonstrated a dual-core SIMT virtual machine on the ESP32 that bridges the gap between microcontroller firmware and massively parallel GPU programming. By leveraging the asymmetric nature of the SoC, we provide a functional platform for learning parallel programming concepts and executing lightweight AI kernels. The ISA v1.5 with true lane-awareness allows for data-parallel algorithms to be written in a CUDA-like assembly, achieving up to 200 MIPS aggregate throughput.

\subsection{Project Achievements}
This work establishes a complete end-to-end toolchain for the ESP32:
\begin{enumerate}
    \item \textbf{Micro-Architecture}: A verified 8-lane SIMD engine with SoA layout and efficient predicated execution.
    \item \textbf{Compiler Stack}: A regex-based LLVM IR frontend and linear-scan register allocator backend (\texttt{mcc.py}).
    \item \textbf{Developer Tools}: Integrated profiling, tracing, and Python-based JIT compilation.
\end{enumerate}

\subsection{Future Roadmap}
Building on this foundation, future development is planned in three phases:

\begin{itemize}
    \item \textbf{Phase 1: Compiler Maturity (Weeks 1-4)}: Focus on implementing automated load/store instruction selection and basic auto-vectorization for SIMT loops.
    \item \textbf{Phase 2: Advanced Features (Months 1-2)}: Introduction of \texttt{\_\_syncthreads()} barriers and shared memory allocation (`.shared`) to support complex inter-lane communication.
    \item \textbf{Phase 3: AI Applications (Months 3+)}: Optimization of SFU functions for Transformer inference and CNN convolution layers, aiming to run lightweight GPT-2 style models directly on the cluster.
\end{itemize}

The Micro-CUDA project demonstrates that modern GPU concepts can be effectively democratized on low-cost standardized hardware, opening new avenues for embedded parallel computing education and research.
