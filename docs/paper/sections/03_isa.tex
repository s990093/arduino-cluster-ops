\section{Micro-CUDA ISA Specification}
\label{sec:isa}

\textbf{Status:} Release Candidate | \textbf{Architecture:} Micro-Cluster (MC) | \textbf{Target:} Deep Learning / Transformer

\subsection{Execution Model \& Architecture Definition}

\subsubsection{Hardware Layer Mapping (Physical Mapping)}

The system implements a three-layer distributed architecture:

\begin{itemize}
    \item \textbf{Layer 1 (Grid Master): AMB82-Mini}
    \begin{itemize}
        \item \textit{Role:} Grid Tiling and DMA Data Injection
        \item \textit{Task:} Responsible for overall grid-level data distribution and kernel launch management
    \end{itemize}
    
    \item \textbf{Layer 2 (SM / Scheduler): ESP32-S3}
    \begin{itemize}
        \item \textit{Role:} Warp Scheduler / Instruction Dispatcher
        \item \textit{Task:} Handles warp scheduling and instruction broadcast
    \end{itemize}
    
    \item \textbf{Layer 3 (Lane / Core): RP2040}
    \begin{itemize}
        \item \textit{Role:} Arithmetic Execution Lane
        \item \textit{Task:} Receives instructions via PIO and executes arithmetic operations
    \end{itemize}
\end{itemize}

\subsubsection{Data Types}

To enable efficient AI inference on the FPU-less RP2040, v2.0 introduces \textbf{Packed BF16}.

\begin{table}[htbp]
\caption{Supported Data Types}
\begin{center}
\begin{tabular}{|l|l|l|p{5cm}|}
\hline
\textbf{Type} & \textbf{Bit Width} & \textbf{Format} & \textbf{Description} \\
\hline
INT32 & 32-bit & 2's Complement & Address calculation, loop counters, indexing \\
FP32 & 32-bit & IEEE 754 & [v1.5] High-precision weights, accumulators \\
BF16 & 16-bit & 1-8-7 (BFloat16) & \textbf{[v2.0]} Same exponent bits as FP32, easy software emulation \\
Packed BF16 & 32-bit & 2× BF16 & \textbf{[v2.0]} High: Element 1, Low: Element 0 \\
INT8 & 8-bit & Signed & [v1.5] Used for \texttt{HMMA.I8} quantized operations \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Register File}

Each Lane (RP2040) independently maintains its own register file:

\begin{itemize}
    \item \textbf{R0 - R31 (General Purpose):} 32× 32-bit registers
    \begin{itemize}
        \item \textit{Compatibility:} Can store INT32, FP32
        \item \textit{Extension:} v2.0 supports \textbf{Packed BF16}
    \end{itemize}
    \item \textbf{P0 - P7 (Predicate):} 8× 1-bit condition flags
    \item \textbf{SR (System Registers):} Read-only status (e.g., \texttt{SR\_LANEID})
\end{itemize}

\subsubsection{Single-Lane Architecture Overview}

Figure \ref{fig:rp2040_simt_arch} illustrates the complete microarchitecture of a single RP2040 lane, showing the data flow from instruction fetch through execution to memory access.

\begin{figure*}[htbp]
\centering
\begin{tikzpicture}[node distance=0.8cm]

% Define professional color scheme (NVIDIA / Academic Style)
\definecolor{coreFill}{RGB}{250, 250, 250}
\definecolor{blockFill}{RGB}{255, 255, 255}
\definecolor{lineColor}{RGB}{50, 60, 70}
\definecolor{headerText}{RGB}{0, 0, 0}
\definecolor{subText}{RGB}{80, 80, 80}
\definecolor{accentGreen}{RGB}{118, 185, 0}
\definecolor{groupBg}{RGB}{242, 245, 248}
\definecolor{borderColor}{RGB}{180, 190, 200}

% Style definitions
\tikzset{
    base/.style={
        rectangle, draw=borderColor, line width=0.8pt, font=\sffamily, align=center
    },
    module/.style={
        base, fill=blockFill, rounded corners=1pt, text width=8.5cm,
        inner sep=8pt, drop shadow={opacity=0.05, shadow xshift=1pt, shadow yshift=-1pt}
    },
    eu/.style={
        base, fill=white, text width=2.6cm, minimum height=3.2cm,
        rounded corners=1pt, font=\sffamily\footnotesize, anchor=north
    },
    labeltext/.style={font=\bfseries\small, text=headerText, align=center},
    code/.style={font=\ttfamily\footnotesize, color=subText},
    wire/.style={
        draw=lineColor, line width=1.0pt, -{Latex[length=2mm, width=1.5mm]}, rounded corners=2pt
    },
    group/.style={
        draw=borderColor, dashed, fill=groupBg, rounded corners=4pt, inner sep=10pt
    }
}

% Title
\node (top_label) [font=\bfseries\large, color=lineColor] {RP2040 SIMT ARCHITECTURE (SINGLE LANE)};

% Frontend / Fetch
\node (frontend) [module, below=0.4cm of top_label] {
    \textbf{Frontend: PIO State Machine} \\[-0.3em]
    \textcolor{borderColor}{\rule{8cm}{0.4pt}} \\[0.3em]
    {\footnotesize Fetch 32-bit Instr (ESP32 @ 50MB/s) $\rightarrow$ Local FIFO Decode}
};

% Register File
\node (regfile) [module, below=0.6cm of frontend] {
    \textbf{Register File (SRAM Bank)} \\[-0.3em]
    \textcolor{borderColor}{\rule{8cm}{0.4pt}} \\[0.2em]
    \begin{tabular}{r l}
        \textbf{\texttt{R0-R31}} & : 32-bit GP \texttt{[High:BF16 | Low:BF16]} \\
        \textbf{\texttt{P0-P7}} & : Predicate Masking \\
        \textbf{\texttt{SR}} & : System State (\texttt{LANE\_ID})
    \end{tabular}
};

% Execution Units
\node (alu_bf16) [eu, below=1.8cm of regfile] {
    \textbf{BF16 Tensor Core} \\
    \textcolor{accentGreen}{\rule{1.5cm}{1pt}} \\
    \vspace{0.2em}
    \begin{tabular}{c}
        BFADD2 \\ BFMUL2 \\ BFMA2 \\ CVT.BF16
    \end{tabular}
};

\node (alu_int) [eu, left=0.3cm of alu_bf16] {
    \textbf{Integer ALU} \\
    \textcolor{lineColor}{\rule{1.5cm}{0.5pt}} \\
    \vspace{0.2em}
    \begin{tabular}{c}
        IADD / IMUL \\ Logic (AND/OR) \\ CMP / ISETP \\ Address Calc
    \end{tabular}
};

\node (sfu) [eu, right=0.3cm of alu_bf16] {
    \textbf{SFU (Trans.)} \\
    \textcolor{lineColor}{\rule{1.5cm}{0.5pt}} \\
    \vspace{0.2em}
    \begin{tabular}{c}
        EXP2 (Softmax) \\ SIN/COS (RoPE) \\ RSQRT (Attn) \\ GELU
    \end{tabular}
};

% Execution core background
\begin{pgfonlayer}{background}
    \node (core_bg) [group, fit=(alu_int)(sfu)(alu_bf16)] {};
    \node [anchor=south west, font=\bfseries\scriptsize, color=lineColor!80] 
        at (core_bg.north west) {EXECUTION DATAPATH};
\end{pgfonlayer}

% Load / Store Unit
\node (lsu) [module, below=1.2cm of core_bg.south] {
    \textbf{Load / Store Unit (LSU)} \\[-0.3em]
    \textcolor{borderColor}{\rule{8cm}{0.4pt}} \\[0.2em]
    {\footnotesize Logic: \texttt{Addr = Base + Offset + (LaneID * 4)}}
    \vspace{0.2em}
    \begin{itemize}
         \setlength\itemsep{0em}
         \scriptsize
         \item[-] \textbf{LDG/STG}: Global Coalesced Access
         \item[-] \textbf{LDL/STL}: Local Thread-Private Access
    \end{itemize}
};

% Memory Hierarchy
\node (mem_sram) [base, fill=white, below=0.8cm of lsu, text width=4cm, xshift=-2.2cm, minimum height=1.5cm] {
    \textbf{Local SRAM} \\
    \scriptsize (Banked / Stack)
};

\node (mem_vram) [base,fill=white, below=0.8cm of lsu, text width=4cm, xshift=2.2cm, minimum height=1.5cm] {
    \textbf{Shared VRAM} \\
    \scriptsize (Weights / KV Cache)
};

\begin{pgfonlayer}{background}
    \node (mem_bg) [group, fit=(mem_sram)(mem_vram), fill=lineColor!5] {};
    \node [anchor=north east, font=\bfseries\scriptsize, color=lineColor!80] 
        at (mem_bg.south east) {MEMORY INTERFACE};
\end{pgfonlayer}

% Connections
\draw [wire] (frontend) -- (regfile);

\coordinate (dispatch_point) at ($(regfile.south) + (0,-0.5)$);
\draw [wire] (regfile.south) -- (dispatch_point);
\draw [wire] (dispatch_point) to[out=-90, in=90] (alu_int.north);
\draw [wire] (dispatch_point) -- (alu_bf16.north);
\draw [wire] (dispatch_point) to[out=-90, in=90] (sfu.north);

\coordinate (collect_y) at ($(core_bg.south) + (0,-0.4)$);

% All arrows point straight down
\draw [line width=1.0pt, draw=lineColor] (alu_int.south) -- (alu_int.south |- collect_y);
\draw [line width=1.0pt, draw=lineColor] (alu_bf16.south) -- (alu_bf16.south |- collect_y);
\draw [line width=1.0pt, draw=lineColor] (sfu.south) -- (sfu.south |- collect_y);

% Horizontal collection bus
\draw [wire] ($(alu_int.south |- collect_y) + (-0.5,0)$) -- ($(sfu.south |- collect_y) + (0.5,0)$);
% Center arrow down to LSU
\draw [wire] ($(alu_bf16.south |- collect_y)$) -- (lsu.north);

\coordinate (mem_split) at ($(lsu.south) + (0,-0.4)$);
\draw [wire] (lsu.south) -- (mem_split);
\draw [wire] (mem_split) to[out=-90, in=90] (mem_sram.north);
\draw [wire] (mem_split) to[out=-90, in=90] (mem_vram.north);

\node [right=0.2cm of regfile, font=\scriptsize, color=accentGreen, rotate=-90] {32-bit Wide};

\end{tikzpicture}
\caption{RP2040 Single-Lane SIMT Microarchitecture. The frontend fetches instructions via PIO, dispatches to execution units (Integer ALU, BF16 Tensor Core, SFU), and accesses memory through the LSU with lane-aware addressing.}
\label{fig:rp2040_simt_arch}
\end{figure*}

\subsection{Instruction Encoding Format}

All Micro-CUDA instructions use a fixed 32-bit encoding to simplify hardware decoding:

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    font=\sffamily\small,
    field/.style={
        rectangle,
        draw=black!70,
        thick,
        minimum height=1.2cm,
        align=center,
        font=\sffamily\footnotesize
    }
]
    % Define field widths (8 bits each = 2cm per field)
    \def\fieldwidth{2.0cm}
    
    % Bit position labels (top)
    \node[font=\scriptsize\ttfamily, anchor=south] at (0, 1.5) {31};
    \node[font=\scriptsize\ttfamily, anchor=south] at (2, 1.5) {24};
    \node[font=\scriptsize\ttfamily, anchor=south] at (2, 1.5) {23};
    \node[font=\scriptsize\ttfamily, anchor=south] at (4, 1.5) {16};
    \node[font=\scriptsize\ttfamily, anchor=south] at (4, 1.5) {15};
    \node[font=\scriptsize\ttfamily, anchor=south] at (6, 1.5) {8};
    \node[font=\scriptsize\ttfamily, anchor=south] at (6, 1.5) {7};
    \node[font=\scriptsize\ttfamily, anchor=south] at (8, 1.5) {0};
    
    % Field boxes with colors
    \node[field, fill=red!20, minimum width=\fieldwidth] (opcode) at (1, 0.6) {\textbf{OPCODE}\\ \scriptsize 8-bit};
    \node[field, fill=blue!20, minimum width=\fieldwidth, right=0cm of opcode] (dest) {\textbf{DEST}\\ \scriptsize Reg Index};
    \node[field, fill=green!20, minimum width=\fieldwidth, right=0cm of dest] (src1) {\textbf{SRC1}\\ \scriptsize Reg Index};
    \node[field, fill=yellow!20, minimum width=\fieldwidth, right=0cm of src1] (src2) {\textbf{SRC2/IMM}\\ \scriptsize Reg or Imm8};
    
    % Bit range labels (bottom)
    \node[font=\scriptsize, below=0.15cm of opcode] {[31:24]};
    \node[font=\scriptsize, below=0.15cm of dest] {[23:16]};
    \node[font=\scriptsize, below=0.15cm of src1] {[15:8]};
    \node[font=\scriptsize, below=0.15cm of src2] {[7:0]};
    
\end{tikzpicture}
\caption{32-bit Instruction Encoding Format. All fields are byte-aligned for efficient parsing.}
\label{fig:instr_encoding}
\end{figure}

\textbf{Field Specifications:}
\begin{itemize}
    \item \textbf{OPCODE [31:24]:} Operation code (256 possible instructions, organized into groups 0x00-0xFF)
    \item \textbf{DEST [23:16]:} Destination register (R0-R31, F0-F31, or P0-P7 depending on instruction type)
    \item \textbf{SRC1 [15:8]:} First source operand register index
    \item \textbf{SRC2/IMM [7:0]:} Second source register \textit{or} 8-bit immediate value (for \texttt{MOV}, \texttt{BRA}, etc.)
\end{itemize}

\subsection{Complete Instruction Set}

\subsubsection{Group 1: System Control (Control \& Flow) [Opcode: 0x00 - 0x0F]}

Handles core flow control, fully compatible with v1.5.

\begin{table*}[htbp]
\caption{System Control Instructions}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0x00} & \textbf{NOP} & - & No operation & v1.5 \\
\texttt{0x01} & \textbf{EXIT} & - & Terminate kernel execution, release resources & v1.5 \\
\texttt{0x02} & \textbf{BRA} & Imm & Unconditional branch (PC += Imm) & v1.5 \\
\texttt{0x03} & \textbf{BR.Z} & Imm, Pn & Branch if Predicate $Pn$ is 0 & v1.5 \\
\texttt{0x05} & \textbf{BAR.SYNC} & Id & Warp Barrier: wait for all lanes to sync & v1.5 \\
\texttt{0x07} & \textbf{YIELD} & - & Yield time slice (cooperative multitasking) & v1.5 \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Group 2: Integer Arithmetic (Integer ALU) [Opcode: 0x10 - 0x1F]}

Handles address calculation and logical control.

\begin{table*}[htbp]
\caption{Integer Arithmetic Instructions}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0x10} & \textbf{MOV} & Rd, Imm & Load immediate value & v1.5 \\
\texttt{0x11} & \textbf{IADD} & Rd, Ra, Rb & Integer addition (address calculation core) & v1.5 \\
\texttt{0x12} & \textbf{ISUB} & Rd, Ra, Rb & Integer subtraction & v1.5 \\
\texttt{0x13} & \textbf{IMUL} & Rd, Ra, Rb & Integer multiplication (32-bit) & v1.5 \\
\texttt{0x17} & \textbf{AND} & Rd, Ra, Rb & Bitwise AND & v1.5 \\
\texttt{0x18} & \textbf{OR} & Rd, Ra, Rb & Bitwise OR & v1.5 \\
\texttt{0x1A} & \textbf{ISETP.EQ} & Pn, Ra, Rb & If Ra == Rb, set Pn = 1 & v1.5 \\
\texttt{0x1C} & \textbf{ISETP.GT} & Pn, Ra, Rb & If Ra $>$ Rb, set Pn = 1 & v1.5 \\
\texttt{0x1D} & \textbf{SHL} & Rd, Ra, Imm & Logical shift left & v1.5 \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Group 3: Deep Learning \& Data Conversion (AI \& Conversion) [Opcode: 0x20 - 0x2F]}

\textbf{} v2.0 additions focused on BF16 and Packed SIMD operations.

\begin{table*}[htbp]
\caption{AI \& Data Conversion Instructions}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0x20} & \textbf{CVT.BF16} & Rd, Ra & \textbf{} Convert FP32 (Ra) to BF16, store in Rd.Low & \textbf{v2.0} \\
\texttt{0x21} & \textbf{CVT.F32} & Rd, Ra & \textbf{} Convert BF16 (Ra.Low) to FP32 (Rd) & \textbf{v2.0} \\
\texttt{0x22} & \textbf{PACK2} & Rd, Ra, Rb & \textbf{} Pack Ra.Low and Rb.Low into Rd & \textbf{v2.0} \\
\texttt{0x25} & \textbf{BFADD2} & Rd, Ra, Rb & \textbf{} Packed BF16 addition (processes High/Low) & \textbf{v2.0} \\
\texttt{0x26} & \textbf{BFMUL2} & Rd, Ra, Rb & \textbf{} Packed BF16 multiplication (processes High/Low) & \textbf{v2.0} \\
\texttt{0x27} & \textbf{BFMA2} & Rd, Ra, Rb & \textbf{} Packed BF16 FMA (\texttt{Rd += Ra * Rb}) & \textbf{v2.0} \\
\texttt{0x28} & \textbf{BFRELU2} & Rd, Ra & \textbf{} Packed BF16 ReLU (\texttt{max(0, x)}) & \textbf{v2.0} \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Group 4: Floating Point \& Transcendental Functions (FP32 \& SFU) [Opcode: 0x30 - 0x5F]}

Integrates v1.5 floating-point instructions with v2.0's complete Math Library.

\begin{table*}[htbp]
\caption{Floating Point \& SFU Instructions}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0x30} & \textbf{FADD} & Fd, Fa, Fb & FP32 addition (IEEE 754) & v1.5 \\
\texttt{0x32} & \textbf{FMUL} & Fd, Fa, Fb & FP32 multiplication & v1.5 \\
\texttt{0x34} & \textbf{FFMA} & Fd, Fa, Fb & FP32 Fused Multiply-Add & v1.5 \\
\texttt{0x40} & \textbf{HMMA.I8} & Rd, Ra, Rb & 4-way INT8 Dot Product (Legacy) & v1.5 \\
\texttt{0x50} & \textbf{SFU.RCP} & Rd, Ra & Reciprocal: $1/x$ (FP32) & v1.5 \\
\texttt{0x51} & \textbf{SFU.EXP2} & Rd, Ra & \textbf{} Base-2 Exp: $2^x$ (BF16/FP32). Softmax key & \textbf{v2.0} \\
\texttt{0x52} & \textbf{SFU.LOG2} & Rd, Ra & \textbf{} Base-2 Log: $\log_2 x$ & \textbf{v2.0} \\
\texttt{0x53} & \textbf{SFU.RSQRT} & Rd, Ra & \textbf{} Fast Inverse Sqrt: $1/\sqrt{x}$. Attention scaling & \textbf{v2.0} \\
\texttt{0x54} & \textbf{SFU.SIN} & Rd, Ra & \textbf{} Sine: $\sin(\pi x)$. RoPE key & \textbf{v2.0} \\
\texttt{0x55} & \textbf{SFU.COS} & Rd, Ra & \textbf{} Cosine: $\cos(\pi x)$. RoPE key & \textbf{v2.0} \\
\texttt{0x56} & \textbf{SFU.GELU} & Rd, Ra & GELU Activation (Fast Tanh approx) & v1.5 \\
\texttt{0x57} & \textbf{SFU.TANH} & Rd, Ra & \textbf{} Tanh Activation & \textbf{v2.0} \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Group 5: Memory Operations [Opcode: 0x60 - 0x7F]}

Core SIMT mechanism supporting Lane-Aware Addressing.

\begin{table*}[htbp]
\caption{Memory Operations}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0x60} & \textbf{LDG} & Rd, [Ra] & \textbf{Uniform Load:} All lanes read same address (Broadcast) & v1.5 \\
\texttt{0x61} & \textbf{STG} & [Ra], Rd & \textbf{Uniform Store:} (usually with atomic ops) & v1.5 \\
\texttt{0x62} & \textbf{LDS} & Rd, [Imm] & \textbf{Shared Load:} Read from Local Shared Memory & v1.5 \\
\texttt{0x63} & \textbf{LDX} & Rd, [Ra+Rb] & \textbf{Indexed Load:} Gather (Ra=Base, Rb=Offset) & v1.5 \\
\texttt{0x64} & \textbf{LDL} & Rd, [Ra] & \textbf{Lane Load:} \texttt{Addr = Ra + (LANEID * 4)}. SIMT core & v1.5 \\
\texttt{0x65} & \textbf{STX} & [Ra+Rb], Rd & \textbf{Indexed Store:} Scatter write & \textbf{v2.0} \\
\texttt{0x67} & \textbf{STL} & [Ra], Rd & \textbf{Lane Store:} \texttt{Addr = Ra + (LANEID * 4)} & v1.5 \\
\texttt{0x70} & \textbf{ATOM.ADD} & [Ra], Rb & Atomic Add (Global/Shared) & v1.5 \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Group 6: System Instructions [Opcode: 0xF0 - 0xFF]}

\begin{table*}[htbp]
\caption{System Instructions}
\begin{center}
\begin{tabular}{|l|l|l|p{8cm}|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Function Description} & \textbf{Compat} \\
\hline
\texttt{0xF0} & \textbf{S2R} & Rd, SRn & System to Register (read Lane ID, etc.) & v1.5 \\
\texttt{0xF1} & \textbf{R2S} & SRn, Rd & \textbf{} Register to System (Debug/Config) & \textbf{v2.0} \\
\texttt{0xF2} & \textbf{TRACE} & Imm & \textbf{} Send Debug Trace Event & \textbf{v2.0} \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Implementation Details: Math Library \& BF16}

To overcome RP2040 hardware limitations, v2.0 instructions are implemented at the Firmware layer (Micro-CUDA VM):

\subsubsection{Packed BF16 Emulation (SIMD2)}

A 32-bit register is treated as \texttt{[ High: BF16\_1 | Low: BF16\_0 ]}.

\textbf{Example: \texttt{BFADD2 Rd, Ra, Rb} C++ Implementation Logic:}

\begin{lstlisting}[language=C++, caption={Firmware Logic for Packed BF16 Addition}]
// Firmware Logic
uint32_t op_bfadd2(uint32_t a, uint32_t b) {
    uint16_t a_lo = a & 0xFFFF; 
    uint16_t a_hi = a >> 16;
    uint16_t b_lo = b & 0xFFFF; 
    uint16_t b_hi = b >> 16;
    
    // Software-emulated BF16 addition
    // Without FPU, only need to handle mantissa/exponent
    uint16_t res_lo = soft_bf16_add(a_lo, b_lo);
    uint16_t res_hi = soft_bf16_add(a_hi, b_hi);
    
    return (res_hi << 16) | res_lo;
}
\end{lstlisting}

\subsubsection{SFU Transcendental Functions (Lookup Tables)}

\begin{itemize}
    \item \textbf{SIN/COS:} Pre-burned 2KB \texttt{sin} lookup table in RP2040 Flash (1024 entries, BF16). Linear interpolation (Lerp) during execution.
    \item \textbf{EXP2:} Uses $2^x$ lookup table. To compute $e^x$, compiler auto-inserts multiplication: $e^x = 2^{x \cdot \log_2 e}$.
    \item \textbf{RSQRT:} Uses BF16-optimized version of Quake III Fast Inverse Square Root algorithm.
\end{itemize}

\subsection{Code Example: v2.0 Softmax Kernel}

This example demonstrates mixing \textbf{SIMT loading (v1.5)} with \textbf{Math Library/BF16 (v2.0)}.

\begin{lstlisting}[language={[x86masm]Assembler}, caption={Softmax Kernel: Exp(x) / Sum(Exp(x))}]
; Kernel: Softmax (Exp(x) / Sum(Exp(x)))
; Input: R0 (Base Address of Input Array, Packed BF16)
; Warp Size: 8 Lanes

; 1. Initialization
S2R     R31, SR_LANEID      ; Get Lane ID

; 2. Load Data (SIMT)
; LDL auto-offsets address: Addr = R0 + LaneID * 4
; Each lane loads 2 BF16 values (Packed)
LDL     R1, [R0]            ; R1 = [x_odd, x_even]

; 3. Compute Exponent (Exp) - using v2.0 SFU
; Convert to Base-2: x * log2(e)
MOV     R2, 0x3FB80000      ; R2 = Packed BF16 constant (log2(e))
BFMUL2  R1, R1, R2          ; Packed Multiply
SFU.EXP2 R3, R1             ; R3 = [2^(x_odd), 2^(x_even)] approx e^x

; 4. Local Sum
; Add packed values, store in FP32 to prevent overflow
CVT.F32 R4, R3              ; R4 = FP32(R3.Low)
SHL     R5, R3, 16          ; Shift High to Low
CVT.F32 R5, R5              ; R5 = FP32(R3.High)
FADD    R6, R4, R5          ; R6 = Local Sum (FP32)

; 5. Warp Reduction (simplified demonstration)
; Typically requires SHFL instruction or Shared Memory data exchange
; Assume R7 contains final warp-wide sum broadcast value

; 6. Normalization
SFU.RCP R8, R7              ; R8 = 1 / TotalSum
CVT.BF16 R8, R8             ; Convert back to BF16
PACK2   R8, R8, R8          ; Duplicate to High/Low: R8 = [1/S, 1/S]

BFMUL2  R9, R3, R8          ; R9 = Exp(x) * (1/Sum) = Softmax(x)

; 7. Write Back
MOV     R10, 0x2000         ; Output Base
STL     [R10], R9           ; Lane-Aware Store
EXIT
\end{lstlisting}

\subsection{SIMT Execution Model Summary}

The Micro-CUDA implementation guarantees the following execution characteristics:

\begin{itemize}
    \item \textbf{True SIMT:} All active lanes execute the same instruction PC in lock-step
    \item \textbf{Lane-Awareness:} Lane IDs are hardware-integrated, removing need for software indexing
    \item \textbf{Independent Registers:} Changes to R/F registers in one lane do not affect others
    \item \textbf{Shared VRAM:} All lanes share a unified 32-bit address space for inter-lane communication
\end{itemize}

\section{Micro-CUDA ISA v2.0 Extensions}
\label{sec:isa_v2}

This section details the "Deep Learning Native" v2.0 specification, designed to address the lack of Math libraries and modern AI data types in the initial implementation. This upgrade enables the ESP32+RP2040 cluster to effectively execute Transformer, RoPE, and Softmax operators.

\subsection{Core Architecture Shifts}
\subsubsection{Native Data Type Expansion (BFloat16)}
Version 2.0 introduces \textbf{BFloat16 (BF16)} as a first-class citizen.
\begin{itemize}
    \item \textbf{Rationale}: BF16 shares the same 8-bit exponent as FP32, allowing conversion via simple truncation without complex bit-shifting or re-biasing. This is critical for the FPU-less Cortex-M0+ (RP2040).
    \item \textbf{Format}: 1 Sign $|$ 8 Exponent $|$ 7 Mantissa (16-bit).
\end{itemize}

\subsubsection{SIMD2 Packed Execution Model}
To maximize 32-bit register utilization, each general-purpose register (`Rx`) is treated as a vector containing two 16-bit BF16 values:
\begin{itemize}
    \item \textbf{R[n].L}: Low 16-bit (Element 0)
    \item \textbf{R[n].H}: High 16-bit (Element 1)
\end{itemize}
This allows a single instruction to process two floating-point numbers simultaneously, effectively doubling the throughput.

\subsection{New Instruction Groups}

\subsubsection{Group 1: Type Conversion}
Bridges the gap between INT8 quantization and FP32 accumulation.

\begin{table}[htbp]
\caption{ISA v2.0 Type Conversion Instructions}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Opcode} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Description} \\
\hline
0x20 & CVT.BF16.F32 & Rd, Ra & FP32 (Ra) $\to$ BF16 (Rd.L) (Truncate) \\
0x21 & CVT.F32.BF16 & Rd, Ra & BF16 (Ra.L) $\to$ FP32 (Rd) (Zero-pad) \\
0x22 & CVT.BF16.I8 & Rd, Ra & 2xINT8 (Ra) $\to$ 2xBF16 (Rd) \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Group 2: BF16 SIMD Arithmetic}
Software emulation of packed BF16 operations.

\begin{table}[htbp]
\caption{ISA v2.0 Packed Arithmetic Instructions}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Op} & \textbf{Mnemonic} & \textbf{Operands} & \textbf{Operation (SIMD2)} \\
\hline
0x25 & BFADD2 & Rd, Ra, Rb & Rd.L/H = Ra.L/H + Rb.L/H \\
0x26 & BFMUL2 & Rd, Ra, Rb & Rd.L/H = Ra.L/H * Rb.L/H \\
0x27 & BFMA2 & Rd, Ra, Rb & Rd += Ra * Rb (Fused) \\
0x28 & BFRELU2 & Rd, Ra & Rd = max(0, Ra) \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Group 3: Special Function Unit (SFU)}
Implements high-precision LUT and Taylor series hybrid algorithms for transcendental functions.

\begin{table}[htbp]
\caption{ISA v2.0 SFU Instructions}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Op} & \textbf{Mnemonic} & \textbf{Func} & \textbf{Use Case} \\
\hline
0x50 & SFU.EXP2 & $2^x$ & Softmax (Numerator) \\
0x51 & SFU.LOG2 & $\log_2(x)$ & Cross Entropy Loss \\
0x52 & SFU.RSQRT & $1/\sqrt{x}$ & Attention Scaling \\
0x53 & SFU.SIN & $\sin(\pi x)$ & RoPE \\
0x54 & SFU.COS & $\cos(\pi x)$ & RoPE \\
0x55 & SFU.TANH & $\tanh(x)$ & GeGLU / LSTM \\
0x56 & SFU.GELU & GELU & Transformer FFN \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Group 4: Tensor Core Operations}
\begin{itemize}
    \item \textbf{BMMA.BF16 (0x45)}: Warp-Level Matrix Multiply ($D = A \times B + C$). Inputs A/B are BF16 vectors; C/D are FP32 accumulators. Optimizes mantissa multiplication using the Cortex-M0+ single-cycle 32x32 multiplier.
\end{itemize}

\subsection{Firmware Implementation Details}
\subsubsection{Fast Exp/Log Strategy}
Instead of computing $e^x$ directly, we calculate $2^x$ by manipulating the exponent field of the BF16 representation. A 3rd-order polynomial fits the mantissa, achieving $< 0.5\%$ error for Softmax.

\subsubsection{Fast RSQRT}
Adapted from the Quake III algorithm for BF16:
\begin{lstlisting}[language=C++, basicstyle=\ttfamily\scriptsize]
uint16_t fast_rsqrt_bf16(uint16_t number) {
    long i;
    // Magic number adjusted for BF16 bias
    i = 0x5F3759DF - ( i >> 1 );
    // ... Newton iteration ...
    return result;
}
\end{lstlisting}

\subsubsection{RoPE SIN/COS LUT}
A 1024-entry BF16 SIN table (2KB) is stored in Flash (XIP). \texttt{SFU.SIN} performs linear interpolation on this table, providing $>10\times$ speedup over Taylor expansion.

\subsection{Micro-Kernel Examples}

\subsubsection{Softmax (SFU.EXP2 + REDUX)}
\begin{lstlisting}[language=MicroCUDA, caption={Softmax Implementation with ISA v2.0}]
; Step 1: Max Reduction
L2R     R2, [R0]        ; Load vector
REDUX.MAX R3, R2        ; Warp-wide Max

; Step 2: Exp and Sum
BSUB2   R4, R2, R3      ; x - max
MUL     R4, R4, 1.44269 ; Convert to base 2
SFU.EXP2 R5, R4         ; 2^(x-max)
REDUX.ADD R6, R5        ; Accumulate sum

; Step 3: Normalize
SFU.RCP R7, R6          ; 1 / Sum
BFMUL2  R8, R5, R7      ; Output
STL     [R1], R8        ; Store
\end{lstlisting}

\subsubsection{RoPE (Rotary Embedding)}
\begin{lstlisting}[language=MicroCUDA, caption={RoPE Implementation}]
SFU.COS R2, R1          ; cos(theta)    
SFU.SIN R3, R1          ; sin(theta)
SHUF.SWAP R4, R0        ; R4 = [x2, x1]
BFMUL2  R5, R0, R2      ; [x1*cos, x2*cos]
BFMUL2  R6, R4, R3      ; [x2*sin, x1*sin]
BFSUB.L R7, R5, R6      ; x1*cos - x2*sin
BFADD.H R7, R5, R6      ; x2*cos + x1*sin
STL     [R_OUT], R7
\end{lstlisting}

\subsection{Practical Example: Transformer Self-Attention}
This comprehensive example demonstrates the complete data flow for computing $\text{Attention}(Q, K) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$ using ISA v2.0 features.

\textbf{Hardware Configuration:}
\begin{itemize}
    \item Warp Size: 8 Lanes (RP2040 cores 0-7)
    \item Data Format: Packed BF16 (2 values per 32-bit register)
    \item Memory Layout: Q, K, V stored as contiguous BF16 arrays in VRAM
\end{itemize}

\begin{lstlisting}[language=MicroCUDA, caption={Transformer Self-Attention Implementation (ISA v2.0)}]
; ===================================================
; Phase 1: Initialization & Lane Identity
; ===================================================
S2R     R31, SR_LANEID      ; R31 = My Lane ID (0..7)

; Base addresses loaded via uniform broadcast from ESP32
; R0 = Base_Q, R1 = Base_K, R2 = Scaling_Factor (1/sqrt(d))

; ===================================================
; Phase 2: SIMT Parallel Load (Packed BF16)
; ===================================================
; LDL performs lane-aware addressing:
; Effective_Addr = Base + (LaneID * 4 bytes)
; Each load fetches 32-bit = 2x BF16 values

LDL     R10, [R0]           ; R10 = Q[2*lane, 2*lane+1]
                            ; Lane 0: Q[0,1], Lane 7: Q[14,15]
LDL     R11, [R1]           ; R11 = K[2*lane, 2*lane+1]

; ===================================================
; Phase 3: Attention Score Computation (Dot Product)
; ===================================================
; Packed multiplication: processes 2 elements per instruction
BFMUL2  R12, R10, R11       ; R12.L = Q[2i]*K[2i]
                            ; R12.H = Q[2i+1]*K[2i+1]

; Note: Full dot product requires warp reduction (omitted here)
; Assume R12 now contains partial score for this lane

; ===================================================
; Phase 4: Scaling (Division by sqrt(d_k))
; ===================================================
BFMUL2  R13, R12, R2        ; R13 = Score * (1/sqrt(d))

; ===================================================
; Phase 5: Softmax Numerator (SFU.EXP2)
; ===================================================
; Compute e^x using base-2 exponentiation
; Formula: e^x = 2^(x * log2(e))

MOV     R4, 0x3FB8          ; BF16 constant: log2(e) = 1.44269
BFMUL2  R13, R13, R4        ; Convert to base 2
SFU.EXP2 R14, R13           ; R14 = [2^Score_L, 2^Score_H]
                            ; Uses Flash LUT + polynomial fitting

; Note: Full Softmax requires sum reduction (omitted)

; ===================================================
; Phase 6: Scatter Store (Lane-Indexed Write)
; ===================================================
MOV     R5, 0x4000          ; Result base address
SHL     R6, R31, 2          ; R6 = LaneID * 4 (byte offset)
STX     [R5 + R6], R14      ; Store to Result[LaneID]

EXIT
\end{lstlisting}

\textbf{Execution Flow Analysis:}
\begin{enumerate}
    \item \textbf{Grid Launch}: AMB82-Mini (Layer 1) initiates kernel dispatch.
    \item \textbf{Instruction Broadcast}: ESP32-S3 (Layer 2) broadcasts \texttt{LDL R10, [R0]} to all 8 lanes via parallel bus.
    \item \textbf{Lane Divergence}:
    \begin{itemize}
        \item Lane 0 (RP2040 \#0): Reads \texttt{SR\_LANEID=0}, fetches from address \texttt{R0 + 0}, loads \texttt{Q[0..1]}.
        \item Lane 7 (RP2040 \#7): Reads \texttt{SR\_LANEID=7}, fetches from address \texttt{R0 + 28}, loads \texttt{Q[14..15]}.
    \end{itemize}
    \item \textbf{Synchronized Execution}: All lanes execute \texttt{SFU.EXP2} simultaneously using firmware LUT (stored in Flash XIP), achieving $>10\times$ speedup vs. Taylor series.
    \item \textbf{Memory Coherence}: Scatter store (\texttt{STX}) ensures each lane writes to its designated slot without conflicts.
\end{enumerate}

\subsection{System Impact Analysis}
\begin{enumerate}
    \item \textbf{Memory Bandwidth}: SIMD2 effectively doubles the memory bandwidth for BF16 weights/activations, mitigating the 8-bit Global G-BUS bottleneck.
    \item \textbf{I-Cache}: \texttt{SFU} instructions replace dozens of ALU instructions, significantly reducing kernel code size and I-Cache pressure.
    \item \textbf{Compiler}: PyTorch \texttt{torch.bfloat16} can now map directly to the ISA without FP32 conversion overhead.
\end{enumerate}
