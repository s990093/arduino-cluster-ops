#!/usr/bin/env python3
"""
ESP32 MicroGPU Image Processing Demo

å®Œæ•´çš„åœ–åƒè™•ç†æµç¨‹ç¯„ä¾‹ï¼š
è¼¸å…¥åœ–ç‰‡ -> Host è™•ç† -> å‚³å…¥ VRAM -> Device é‹ç®— -> å–å›çµæœ -> é¡¯ç¤º

Features:
- CUDA-style API (malloc, memcpy, launch)
- Tile-based execution for 8-lane SIMT
- Real-time visualization with Matplotlib
"""

import sys
import time
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Tuple, Optional
from PIL import Image
import torch
import torch.nn.functional as F
import re
import os

sys.path.insert(0, str(Path(__file__).parent.parent))
from esp32_tools import ESP32Connection
from esp32_tools.program_loader_v15 import InstructionV15


class MicroGPU:
    """
    MicroGPU: CUDA-style API for ESP32 CUDA VM
    
    æ¨¡æ“¬ CUDA ç·¨ç¨‹æ¨¡å‹:
    - cudaMalloc() -> malloc()
    - cudaMemcpy() -> memcpy()
    - kernel<<<grid, block>>>() -> launch()
    """
    
    def __init__(self):
        # Configuration
        PORT = "/dev/cu.usbserial-589A0095521"
        # BAUD_RATE = 921600
        BAUD_RATE = 115200 # Downgraded for stability
        TIMEOUT = 120 # Increased timeout for slow baud
        VM_VRAM_SIZE = 40960 # Matches Firmware Config

        # Execution Params
        # BYTES_PER_UPLOAD = 2048
        BYTES_PER_UPLOAD = 256 # Safe chunk size
        """åˆå§‹åŒ– MicroGPU è¨­å‚™"""
        self.conn = ESP32Connection(PORT, baudrate=BAUD_RATE)
        self.vram_allocator = 0  # VRAM åˆ†é…å™¨ï¼ˆå­—ç¯€åœ°å€ï¼‰
        self.allocations = {}     # è¨˜éŒ„å·²åˆ†é…çš„è¨˜æ†¶é«”
        self.program_loaded = False  # è¿½è¹¤ç¨‹åºæ˜¯å¦å·²åŠ è¼‰
        
        # Reset device
        self.conn.send_command("reset", delay=0.5)
        print("ğŸ® MicroGPU Device Initialized")
    
    def malloc(self, name: str, size_bytes: int) -> int:
        """
        åœ¨ VRAM ä¸­åˆ†é…è¨˜æ†¶é«”
        
        Args:
            name: è¨˜æ†¶é«”å€åŸŸåç¨±ï¼ˆç”¨æ–¼è¿½è¹¤ï¼‰
            size_bytes: éœ€è¦çš„å­—ç¯€æ•¸
            
        Returns:
            åˆ†é…çš„èµ·å§‹åœ°å€
        """
        addr = self.vram_allocator
        self.allocations[name] = {
            'addr': addr,
            'size': size_bytes
        }
        self.vram_allocator += size_bytes
        
        # å°é½Šåˆ° 4 å­—ç¯€
        if self.vram_allocator % 4 != 0:
            self.vram_allocator += (4 - self.vram_allocator % 4)
        
        print(f"  ğŸ“¦ malloc('{name}'): {size_bytes} bytes @ 0x{addr:04X}")
        return addr
    
    def memcpy_host_to_device(self, name: str, data: np.ndarray) -> None:
        """
        å¾ Host è¤‡è£½æ•¸æ“šåˆ° Device VRAM
        
        Args:
            name: ç›®æ¨™è¨˜æ†¶é«”å€åŸŸåç¨±
            data: NumPy æ•¸çµ„ï¼ˆuint8 æˆ– int32ï¼‰
        """
        if name not in self.allocations:
            raise ValueError(f"Memory '{name}' not allocated")
        
        addr = self.allocations[name]['addr']
        
        # ç¢ºä¿æ•¸æ“šæ˜¯ int32
        if data.dtype != np.int32:
            data = data.astype(np.int32)
        
        # å¯«å…¥ VRAM
        for i, val in enumerate(data.flat):
            # å¢åŠ  delay ä»¥ç¢ºä¿ Arduino æœ‰è¶³å¤ æ™‚é–“è™•ç†
            self.conn.send_command(f"mem {addr + i * 4} {int(val)}", delay=0.05)
        
        print(f"  â¬‡ï¸  memcpy H->D: '{name}' ({len(data.flat)} elements)")
    
    def memcpy_device_to_host(self, name: str, shape: Tuple[int, ...]) -> np.ndarray:
        """
        å¾ Device VRAM è®€å–æ•¸æ“šåˆ° Host
        
        å®Œæ•´çš„ VRAM æµç¨‹ï¼šå¾æŒ‡å®š VRAM åœ°å€è®€å–è¨ˆç®—çµæœ
        
        Args:
            name: æºè¨˜æ†¶é«”å€åŸŸåç¨±  
            shape: è¼¸å‡ºæ•¸çµ„å½¢ç‹€
            
        Returns:
            NumPy æ•¸çµ„
        """
        if name not in self.allocations:
            raise ValueError(f"Memory '{name}' not allocated")
        
        addr = self.allocations[name]['addr']
        size = np.prod(shape)
        
        # æ¸…é™¤ä¹‹å‰çš„è¼¸å‡º (å¦‚ Mem Written, Loaded ç­‰)
        # é¿å… dump çµæœè¢«æ·¹æ²’æˆ–è®€ä¸åˆ°
        _ = self.conn.read_lines()
        
        # ä½¿ç”¨ dump å¾ VRAM è®€å–çµæœ
        import time
        import re
        
        # ç™¼é€å‘½ä»¤ (delay ç¨å¾®ä¿ç•™ä¸€é»ï¼Œä½†ä¸»è¦é  polling)
        self.conn.send_command(f"dump {addr} {size}", delay=0.1)
        
        result = []
        start_time = time.time()
        timeout = 5.0  # 5ç§’è¶…æ™‚
        
        print(f"     â³ Polling for result ({size} items)...")
        
        raw_log = [] # è¨˜éŒ„æ‰€æœ‰æ”¶åˆ°çš„è¡Œä»¥ä¾¿èª¿è©¦
        
        # Clear buffer first
        self.conn.ser.reset_input_buffer()
        
        while len(result) < size and (time.time() - start_time < timeout):
            lines = self.conn.read_lines()
            for line in lines:
                clean_line = line.strip()
                raw_log.append(clean_line)
                
                # åªæ¥å— 4ä½16é€²åˆ¶åœ°å€ + å†’è™Ÿ + æ•¸å­— çš„æ ¼å¼
                match = re.match(r'^([0-9a-fA-F]{4}):\s+(\d+)$', clean_line)
                if match:
                    val = int(match.group(2))
                    result.append(val)
            
            if len(result) < size:
                time.sleep(0.1)
        
        # å¦‚æœçµæœæ•¸é‡ä¸å°ï¼Œæ‰“å°åŸå§‹è¼¸å‡ºä»¥ä¾¿èª¿è©¦
        if len(result) < size:
            print(f"  âš ï¸  Warning: Expected {size} values, got {len(result)}")
            print(f"  âš ï¸  Last 20 raw lines captured:")
            for l in raw_log[-20:]:
                print(f"      {l}")
        
        # å¡«å……åˆ°æ‰€éœ€å¤§å°
        while len(result) < size:
            result.append(0)
        
        # è™•ç†æº¢å‡ºï¼šå°‡å€¼é™åˆ¶åœ¨ 0-255 ç¯„åœ
        clamped_result = []
        for val in result[:size]:
            # è™•ç†ç„¡ç¬¦è™Ÿ 32 ä½è½‰æœ‰ç¬¦è™Ÿ
            if val > 2147483647:  # å¤§æ–¼ int32 æœ€å¤§å€¼ï¼Œèªªæ˜æ˜¯è² æ•¸
                val = val - 4294967296  # è½‰æ›ç‚ºæœ‰ç¬¦è™Ÿè² æ•¸
            # å–çµ•å°å€¼ (å› ç‚º kernel æ²’æœ‰ ABS æŒ‡ä»¤)
            val = abs(val)
            # Clamp åˆ° 0-255
            clamped_result.append(max(0, min(255, val)))
        
        if len(clamped_result) != size:
            print(f"  âš ï¸  Size Mismatch in cudaMemcpy! Expected {size}, Got {len(clamped_result)}. Truncating/Padding.")
            if len(clamped_result) > size:
                clamped_result = clamped_result[:size]
            else:
                while len(clamped_result) < size:
                     clamped_result.append(0)

        data = np.array(clamped_result, dtype=np.int32).reshape(shape)
        print(f"  â¬†ï¸  memcpy D->H: '{name}' @ 0x{addr:04X} ({len(result)} checked -> {len(data.flatten())} kept)")
        return data
    
    def launch(self, kernel_code: List, grid_size: int = 1, block_size: int = 8) -> None:
        """
        å•Ÿå‹• Kernel åŸ·è¡Œ
        
        Args:
            kernel_code: æŒ‡ä»¤åˆ—è¡¨ï¼ˆInstructionV15ï¼‰
            grid_size: Grid å¤§å°ï¼ˆæ¨¡æ“¬å¤šæ¬¡åŸ·è¡Œï¼‰
            block_size: Block å¤§å°ï¼ˆWarp Sizeï¼Œå›ºå®šç‚º 8ï¼‰
        """
        # åªåœ¨ç¬¬ä¸€æ¬¡åŠ è¼‰ç¨‹åº
        if not self.program_loaded:
            for inst in kernel_code:
                self.conn.send_command(f"load {inst.to_hex()}", delay=0.01)
            self.program_loaded = True
            print(f"  ğŸ“ Loaded {len(kernel_code)} instructions")
        
        print(f"  ğŸš€ launch<<<{grid_size}, {block_size}>>>: execute")
        
        # åŸ·è¡Œï¼ˆå°æ–¼ grid_size > 1ï¼Œéœ€è¦å¤šæ¬¡åŸ·è¡Œä¸¦èª¿æ•´ offsetï¼‰
        for grid_idx in range(grid_size):
            self.conn.send_command("run", delay=0.5)
            print(f"     Grid[{grid_idx}/{grid_size}] executed")
    
    def free_all(self) -> None:
        """é‡‹æ”¾æ‰€æœ‰åˆ†é…çš„è¨˜æ†¶é«”"""
        self.allocations.clear()
        self.vram_allocator = 0
        self.program_loaded = False
        print("  ğŸ—‘ï¸  All memory freed")


def create_test_kernel() -> List:
    """
    Simple Test Kernel: Write Lane ID to Output
    
    This verifies that:
    1. s2r(SR_LANEID) works correctly  
    2. Each lane has unique ID (0-7)
    3. stx writes to correct addresses
    
    Expected Output @ 0x4000: [0, 1, 2, 3, 4, 5, 6, 7]
    
    Structure matches edge_detection_kernel for dynamic patching:
    [0]: s2r
    [1]: mov(10, 0)  - Input base (will be replaced)
    [2]: mov(11, 0)  - Output base (will be replaced)
    [3:]: Core logic
    """
    kernel = [
        # [0] R31 = lane_id
        InstructionV15.s2r(31, InstructionV15.SR_LANEID),
        
        # [1] R10 = Input Base (not used, but needed for patching structure)
        InstructionV15.mov(10, 0),
        
        # [2] R11 = Output Base (will be replaced by load_register_32bit)
        InstructionV15.mov(11, 0),
        
        # [3:] Core logic starts here
        # R20 = 4 (word size)
        InstructionV15.mov(20, 4),
        
        # R21 = lane_id * 4 (byte offset)
        InstructionV15.imul(21, 31, 20),
        
        # Write lane_id to output[lane_id]
        # stx(base_reg, offset_reg, src_data)
        InstructionV15.stx(11, 21, 31),  # [R11 + R21] = R31
        
        # Exit
        InstructionV15.exit_inst()
    ]
    return kernel


def create_edge_detection_kernel() -> List:
    """
    ä¿®æ­£å¾Œçš„é‚Šç·£æª¢æ¸¬ Kernel (v2)
    
    Features:
    1. Lane 0 Guard: ä½¿ç”¨æ¢ä»¶é‹ç®—é˜²æ­¢è¶Šç•Œ
    2. Absolute Value: è¨ˆç®— |curr - prev|
    
    Memory Layout:
    - 0x0000: è¼¸å…¥åœ–åƒæ•¸æ“š (8 pixels)
    - 0x0020: è¼¸å‡ºé‚Šç·£æ•¸æ“š (8 pixels)
    """
    kernel = [
        # ===== åˆå§‹åŒ– =====
        InstructionV15.s2r(31, InstructionV15.SR_LANEID),  # R31 = lane_id
        InstructionV15.mov(10, 0),      # R10 = Input Base
        InstructionV15.mov(11, 8192),   # R11 = Output Base (Offset for 64x128 chunk)
        InstructionV15.mov(20, 4),      # R20 = 4
        
        # ===== è¨ˆç®—ç•¶å‰åƒç´ åœ°å€ =====
        InstructionV15.imul(21, 31, 20),  # R21 = lane_id * 4
        InstructionV15.ldx(0, 10, 21),    # R0 = current pixel
        
        # ===== Lane 0 Guard: å¦‚æœ lane_id == 0ï¼Œè¨­ previous = current =====
        InstructionV15.mov(1, 0),         # R1 = 0 (é»˜èª previous)
        
        # å¦‚æœ lane_id > 0ï¼ŒR1 = input[lane_id-1]
        # è¨ˆç®—å‰ä¸€å€‹åœ°å€
        InstructionV15.mov(22, 1),
        InstructionV15.isub(23, 31, 22),  # R23 = lane_id - 1
        InstructionV15.imul(24, 23, 20),  # R24 = (lane_id-1) * 4
        
        # è®€å–å‰ä¸€å€‹å€¼ (å³ä½¿æ˜¯ Lane 0 ä¹Ÿè®€ï¼Œé€™æ˜¯å†’éšªçš„ï¼Œä½†ä¹‹å‰é‚è¼¯æ˜¯é€™æ¨£)
        # æ³¨æ„ï¼šLane 0 è®€å– -4 å¯èƒ½å´©æ½°æˆ–è®€åˆ°åƒåœ¾ï¼Ÿ
        # å¦‚æœ VM æ²’æœ‰ä¿è­·ï¼Œé€™å¾ˆå±éšªã€‚
        # åŸé‚è¼¯ç›´æ¥è®€å– R24ã€‚å¦‚æœæ˜¯ -4 (FFFFFFFC)ï¼Œå¯èƒ½è®€åˆ°éæ³•åœ°å€ã€‚
        # è®“æˆ‘ä¿®æ”¹é‚è¼¯é¿å…è®€å–éæ³•åœ°å€ã€‚
        
        # å®‰å…¨è®€å–é‚è¼¯:
        # å¦‚æœ lane_id == 0ï¼ŒR24 = 0 (è®€å–è‡ªå·±)
        # ç”¨ä¹˜æ³•æ¨¡æ“¬æ¢ä»¶: R24 = R24 * (lane_id != 0) ??? é›£ä»¥å¯¦ç¾
        
        # ç°¡å–®æ–¹æ³•: æ—¢ç„¶ R1 åˆå§‹åŒ–ç‚º 0ã€‚
        # åªæœ‰ç•¶ lane_id > 0 æ™‚æ‰åŸ·è¡Œ ldx? ä¸æ”¯æŒæ¢ä»¶åŸ·è¡Œã€‚
        
        # è®“æˆ‘å€‘æ¢å¾©åŸé‚è¼¯ä¸¦è§€å¯Ÿã€‚åŸé‚è¼¯:
        InstructionV15.ldx(1, 10, 24),    # R1 = input[lane_id-1]
        
        # å¦‚æœ Lane 0ï¼Œé€™æœƒè®€å– Mem[-4]ã€‚
        # å¦‚æœé€™æ˜¯å•é¡Œæ‰€åœ¨ï¼Ÿ
        # åœ¨ debug_vram æ¸¬è©¦ä¸­ï¼ŒLane 0 è®€å– Mem[0]ã€‚
        # é€™è£¡ R23 = 0 - 1 = -1. R24 = -4.
        
        # å¦‚æœ Lane 0 è®€äº†åƒåœ¾ï¼Œä¸”å¾Œé¢é‚è¼¯è©¦åœ–ä¿®æ­£ï¼š
        # InstructionV15.mov(25, 0), isub, ...
        # é€™åªæ˜¯ç‚ºäº†é¸æ“‡ R1 çš„å€¼ã€‚
        
        # ä¿®æ”¹ï¼šç¢ºä¿ Lane 0 ä¸è®€å–è¶Šç•Œã€‚
        # å°‡ R24 é™åˆ¶ç‚º >= 0?
        # ç„¡æ³•ç°¡å–®åšåˆ°ã€‚
        
        # ä½†æ˜¯ï¼Œå¦‚æœ Lane 0 è®€å– -4 æ²’å´©æ½°ï¼Œåªæ˜¯è®€äº†åƒåœ¾ã€‚
        # ç„¶å¾Œæˆ‘å€‘è¦†è“‹ R1ã€‚
        
        # é€™è£¡æˆ‘å€‘å˜—è©¦ç”¨ R0 è¦†è“‹ R1 (å¦‚æœæ˜¯ Lane 0)
        # æˆ‘å€‘ä¹‹å‰ç¢ºå¯¦æœ‰é€™å€‹é‚è¼¯å—ï¼Ÿ
        # åŸä»£ç¢¼ï¼š
        # if lane_id == 0, R1 = R0 (ç”¨ç•¶å‰è¦†è“‹)
        # ä½†å¦‚ä½•å¯¦ç¾ï¼Ÿ
        # ä¹‹å‰çš„ä»£ç¢¼æ²’æœ‰é¡¯ç¤ºå…·é«”çš„ "if lane_id == 0" å¯¦ç¾ï¼Œåªæœ‰æ³¨é‡‹ã€‚
        # "ç®—è¡“æŠ€å·§... ç°¡åŒ–... æ¥å—èª¤å·®".
        
        # è®“æˆ‘å˜—è©¦ä¸€å€‹æ›´å®‰å…¨çš„é‚è¼¯:
        # ç¸½æ˜¯è®€å– R0 (ç•¶å‰)ã€‚
        # åªæœ‰ç•¶ lane_id > 0 æ™‚ï¼Œè®€å– R1 (å‰ä¸€å€‹)ã€‚
        # ===== è¨ˆç®—æ¢¯åº¦ diff =====
        # R2 = current - previous
        # æ³¨æ„ï¼šå°æ–¼ Lane 0ï¼Œprevious (R1) ç‚º 0ï¼Œå› æ­¤ R2 = current
        InstructionV15.isub(2, 0, 1),
        
        # ===== å¢å¼·å°æ¯” (Scale * 3) =====
        # ä½¿ç”¨ R25 å­˜å„²å¸¸æ•¸ 3ï¼Œé¿å…ä½¿ç”¨ R3
        InstructionV15.mov(25, 3),        # R25 = 3
        InstructionV15.imul(2, 2, 25),    # R2 = R2 * 3
        
        # ===== å¯«å›VRAM =====
        # Python ç«¯æœƒè™•ç†ç¬¦è™Ÿ (signed/unsigned) å’Œçµ•å°å€¼
        InstructionV15.stx(11, 21, 2),    # output[lane_id] = R2
        
        InstructionV15.exit_inst()
    ]
    return kernel



def process_image_with_microgpu(
    image_path: str,
    gpu: MicroGPU,
    tile_size: int = 8
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨ MicroGPU è™•ç†åœ–åƒ (Tile-based Grid Execution)
    
    å› ç‚ºç¡¬é«”é™åˆ¶ (Warp Size = 8)ï¼Œå¿…é ˆç”± Host æ‰‹å‹•åˆ‡åˆ†åœ–åƒï¼Œ
    æ¯æ¬¡è™•ç† 8 å€‹åƒç´  (ä¸€å€‹ tile)ã€‚
    
    Args:
        image_path: è¼¸å…¥åœ–åƒè·¯å¾‘
        gpu: MicroGPU å¯¦ä¾‹
        tile_size: åˆ†å¡Šå¤§å°ï¼ˆå›ºå®šç‚º 8ï¼Œå°æ‡‰ Warp Sizeï¼‰
        
    Returns:
        (åŸåœ–, è™•ç†å¾Œçš„åœ–åƒ)
    """
    print("\n" + "=" * 70)
    print("ğŸ–¼ï¸  Image Processing with MicroGPU (128x128 Optimized)")
    print("=" * 70)
    
    # 1. è¼‰å…¥åœ–åƒ -> Use Synthetic Gradient for Debugging
    # img = Image.open(image_path).convert('L')
    # img = img.resize((target_size, target_size), Image.LANCZOS)
    
    print(f"ğŸ§ª Generating Synthetic Gradient Image for Debugging...")
    target_size = 32 # User Request: 32x32 First
    # Gradient from 0 to 255 horizontally
    img_array = np.zeros((target_size, target_size), dtype=np.int32)
    for y in range(target_size):
        for x in range(target_size):
            img_array[y, x] = (x * 255) // target_size
            
    print(f"ğŸ“ Synthetic Image {target_size}x{target_size} created")
    
    h, w = img_array.shape
    
    print(f"ğŸ“¥ Loaded image: {img_array.shape}")
    print(f"ğŸ”¢ Total pixels: {h * w}")
    
    # 2. åˆ†é… VRAM (æ¯è¡Œä¸€å€‹ç·©è¡å€)
    # ç‚ºäº†å„ªåŒ–ï¼Œæˆ‘å€‘è™•ç†ä¸€æ•´è¡Œ (128 pixels)
    # VRAM Layout:
    # 0x0000: Input Row Buffer (128 * 4 = 512 bytes)
    # 0x0200: Output Row Buffer (128 * 4 = 512 bytes)
    
    # æ¸…é™¤ä¹‹å‰çš„åˆ†é… (å¦‚æœæœ‰)
    gpu.allocations.clear()
    gpu.vram_allocator = 0
    
    print("\nğŸ’¾ Allocating VRAM buffers...")
    input_base = 0x0000
    output_base = 0x0200
    
    # ä¸éœ€è¦çœŸæ­£ mallocï¼Œç›´æ¥ä½¿ç”¨å›ºå®šåœ°å€ä»¥é…åˆ patched kernel
    gpu.allocations["row_input"] = {'addr': input_base, 'size': w * 4}
    gpu.allocations["row_output"] = {'addr': output_base, 'size': w * 4}
    
    # 3. å‰µå»º Kernel
    print("\nâš™ï¸  Compiling kernel...")
    kernel = create_edge_detection_kernel()
    
    # 4. æº–å‚™è¼¸å‡ºåœ–åƒ
    output_array = np.zeros_like(img_array)
    
    # 2. åˆ†é… VRAM
    CHUNK_ROWS = 32
    CHUNK_SIZE_PIXELS = CHUNK_ROWS * w
    CHUNK_SIZE_BYTES = CHUNK_SIZE_PIXELS * 4
    
    input_base = 0x0000
    output_base = 0x4000 # 16KB offset
    
    print(f"\nğŸ’¾ Allocating VRAM buffers (Chunk-based)...")
    print(f"  Chunk Size: {CHUNK_SIZE_BYTES} bytes ({CHUNK_ROWS} rows)")
    print(f"  Input Base: 0x{input_base:X}")
    print(f"  Output Base: 0x{output_base:X}")
    
    # helper to print progress bar
    def print_progress(current, total, prefix='', suffix='', decimals=1, length=50, fill='â–ˆ', printEnd="\r"):
        percent = ("{0:." + str(decimals) + "f}").format(100 * (current / float(total)))
        filledLength = int(length * current // total)
        bar = fill * filledLength + '-' * (length - filledLength)
        print(f'\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)
        if current == total: 
            print()
            
    # helper to load 32-bit immediate into register (using 8-bit MOVs)
    def load_register_32bit(reg, val):
        # Algorithm:
        # R = (B3 << 24) | (B2 << 16) | (B1 << 8) | B0
        # 1. MOV R, B3
        # 2. MOV T, 8 (Shift)
        # 3. SHL R, R, T
        # 4. MOV T, B2
        # 5. OR R, R, T
        # 6. SHL R, R, Tm (using 8) ...
        # Optimization: Use fixed temp reg R26 for value, R27 for shift=8
        insts = []
        b0 = val & 0xFF
        b1 = (val >> 8) & 0xFF
        b2 = (val >> 16) & 0xFF
        b3 = (val >> 24) & 0xFF
        
        # Load High Byte
        insts.append(InstructionV15.mov(reg, b3))
        
        # Setup Shift Reg (R27 = 8)
        insts.append(InstructionV15.mov(27, 8))
        
        # Shift & Add B2
        insts.append(InstructionV15.shl(reg, reg, 27))
        insts.append(InstructionV15.mov(26, b2))
        insts.append(InstructionV15.or_op(reg, reg, 26))
        
        # Shift & Add B1
        insts.append(InstructionV15.shl(reg, reg, 27))
        insts.append(InstructionV15.mov(26, b1))
        insts.append(InstructionV15.or_op(reg, reg, 26))
        
        # Shift & Add B0
        insts.append(InstructionV15.shl(reg, reg, 27))
        insts.append(InstructionV15.mov(26, b0))
        insts.append(InstructionV15.or_op(reg, reg, 26))
        
        return insts

    output_array = np.zeros_like(img_array)
    import time
    start_time = time.time()
    
    # Chunk Processing
    # Chunk Processing
    num_chunks = h // CHUNK_ROWS
    for chunk_idx in range(num_chunks):
        print(f"\nğŸ“¦ Processing Chunk {chunk_idx+1}/{num_chunks}...")
        
        row_start = chunk_idx * CHUNK_ROWS
        row_end = row_start + CHUNK_ROWS
        chunk_data = img_array[row_start:row_end, :].flatten()
        
        # Clear VRAM before first chunk to remove stale data
        if chunk_idx == 0:
            print("  ğŸ§¹ Clearing VRAM...")
            gpu.conn.send_command("reset", delay=0.5)
        
        # 1. Bulk Upload
        print("  â¬†ï¸  Uploading data...")
        BYTES_PER_UPLOAD = 128 # Reduced to fit safe serial buffer
        total_bytes = len(chunk_data) * 4
        uploaded_bytes = 0
        
        while uploaded_bytes < total_bytes:
            pixels_per_batch = BYTES_PER_UPLOAD // 4
            batch_start_idx = uploaded_bytes // 4
            batch_end_idx = min(len(chunk_data), batch_start_idx + pixels_per_batch)
            batch_pixels = chunk_data[batch_start_idx:batch_end_idx]
            if len(batch_pixels) == 0: break
            
            import struct
            hex_data = ""
            for val in batch_pixels:
                packed = struct.pack('<I', int(val))
                hex_data += packed.hex()
                
            # Debug: Print first batch data sample
            if uploaded_bytes == 0:
                 print(f"      [Debug] Host Data Sample: {batch_pixels[:4]}")

            curr_addr = input_base + uploaded_bytes
            gpu.conn.send_command(f"wbulk {curr_addr} {len(hex_data)//2} {hex_data}", delay=0.01)
            uploaded_bytes += len(hex_data) // 2
            
            # Progress Bar
            print_progress(uploaded_bytes, total_bytes, prefix='Upload:', length=30)
            
        print("    âœ… Complete")
        
        # Verify Input Upload
        print("    ğŸ” Verifying Input VRAM (First 4 pixels)...")
        # Reuse dump logic manual
        gpu.conn.ser.reset_input_buffer()
        check_count = 4
        gpu.conn.send_command(f"dump {input_base} {check_count}", delay=0.5)
        check_res = []
        raw_debug = []
        st = time.time()
        while len(check_res) < check_count and (time.time() - st < 5.0):
             ls = gpu.conn.read_lines()
             for l in ls:
                 raw_debug.append(l)
                 match = re.match(r'^([0-9a-fA-F]{4,8}):\s+(\d+)$', l.strip())
                 if match: check_res.append(int(match.group(2)))
        print(f"      [Debug] VRAM Input Sample: {check_res}")
        if not check_res:
            print(f"      [Debug] Raw Dump Output: {raw_debug}")

        # 2. Executing Kernels
        print("  ğŸš€ Executing kernels...")
        total_blocks = CHUNK_ROWS * (w // 8)
        processed_blocks = 0
        
        for r in range(CHUNK_ROWS):
            row_offset = r * w * 4 
            for block in range(w // 8):
                block_offset = block * 32
                curr_input = input_base + row_offset + block_offset
                curr_output = output_base + row_offset + block_offset
                
                # Dynamic Kernel Generation (with 32-bit address)
                # é‡æ–°ç”Ÿæˆ Kernelï¼Œä½†æ˜¯å°‡é–‹é ­çš„ Base Address è¨­ç½®éƒ¨åˆ†æ›¿æ›
                # åŸå§‹ create_edge_detection_kernel è¿”å›:
                # [0]: s2r(31, laneid)
                # [1]: mov(10, 0)  <-- Replace
                # [2]: mov(11, 32) <-- Replace
                # ...
                
                # Back to edge detection kernel
                base_kernel = create_edge_detection_kernel()
                # Remove [1] and [2]
                core_logic = [base_kernel[0]] + base_kernel[3:]
                
                # Insert Address Loaders
                load_in  = load_register_32bit(10, curr_input)
                load_out = load_register_32bit(11, curr_output)
                
                # Combine: [S2R] + [Load In] + [Load Out] + [Core Logic]
                full_kernel = [base_kernel[0]] + load_in + load_out + base_kernel[3:]
                
                # Send command bundle
                # Use softreset to reset VM logic but KEEP VRAM Data!
                cmd_chunk = "softreset\n" 
                kernel_hex_list = []
                for inst in full_kernel:
                    encoded = inst.encode()
                    hex_str = f"{encoded:x}"
                    kernel_hex_list.append(hex_str)
                    cmd_chunk += f"load {hex_str}\n"
                
                # Debug: log kernel
                if False and r == 0 and block == 0:  # Disabled debug output
                    print(f"      [Debug] curr_input=0x{curr_input:X}, curr_output=0x{curr_output:X}")
                    print(f"      [Debug] Full kernel ({len(kernel_hex_list)} instructions):")
                    for i, hex_str in enumerate(kernel_hex_list):
                        print(f"        [{i:2d}] {hex_str}")
                
                cmd_chunk += f"run\n"
                gpu.conn.ser.write(cmd_chunk.encode())
                
                # Debug: register verification (DISABLED for clean output)
                if False and r == 0 and block == 0:
                    # The kernel was just loaded and run command sent
                    # Wait for execution to complete
                    time.sleep(0.1)
                    # Now dump registers to see what R11 was during execution  
                    for lane_idx in range(8):
                        gpu.conn.send_command(f"reg {lane_idx}", delay=0.15)
                        reg_lines = gpu.conn.read_lines()
                        has_nonzero = False
                        lane_regs = []
                        for line in reg_lines:
                            if "R2 " in line or "R11" in line or "R21" in line or "R31" in line:
                                lane_regs.append(line.strip())
                                if " = " in line and line.split("=")[1].strip() != "0":
                                    has_nonzero = True
                        if has_nonzero:
                            print(f"      [Debug Lane {lane_idx}]")
                            for reg in lane_regs:
                                print(f"        {reg}")
                
                # Need delay? 
                # 30 lines * 10 bytes = 300 bytes. 921600bps -> 3ms.
                # Processing 8 pixels -> very fast.
                time.sleep(0.005) 
                
                processed_blocks += 1
                if processed_blocks % 8 == 0:
                     print_progress(processed_blocks, total_blocks, prefix='Exec:  ', length=30)
                     
        print_progress(total_blocks, total_blocks, prefix='Exec:  ', length=30)
        print("    âœ… Complete")
        
        # Debug: Verify Output Buffer (DISABLED for clean output)
        if False:
            print("    ğŸ” Verifying Output VRAM (First 4 pixels after execution)...")
            gpu.conn.ser.reset_input_buffer()
            gpu.conn.send_command(f"dump {output_base} 4", delay=0.5)
            check_out = []
            raw_out = []
            st = time.time()
            while len(check_out) < 4 and (time.time() - st < 5.0):
                 ls = gpu.conn.read_lines()
                 for l in ls:
                     raw_out.append(l)
                     match = re.match(r'^([0-9a-fA-F]{4,8}):\s+(\d+)$', l.strip())
                     if match: check_out.append(int(match.group(2)))
            print(f"      [Debug] VRAM Output Sample (@ 0x{output_base:X}): {check_out}")
            if not check_out:
                print(f"      [Debug] Raw Output Dump: {raw_out}")
        
        # Input verification (DISABLED)
        if False:
            print("    ğŸ” Re-checking Input VRAM after execution...")
            gpu.conn.ser.reset_input_buffer()
            gpu.conn.send_command(f"dump {input_base} 4", delay=0.5)
            check_in2 = []
            st = time.time()
            while len(check_in2) < 4 and (time.time() - st < 3.0):
                 ls = gpu.conn.read_lines()
                 for l in ls:
                     match = re.match(r'^([0-9a-fA-F]{4,8}):\s+(\d+)$', l.strip())
                     if match: check_in2.append(int(match.group(2)))
            print(f"      [Debug] Input still @ 0x{input_base:X}: {check_in2}")
        
        # 3. Download Results
        print("  â¬‡ï¸  Downloading results...")
        chunk_result = []
        downloaded = 0
        
        for i in range(0, total_bytes, BYTES_PER_UPLOAD):
            curr_addr = output_base + i
            size = min(BYTES_PER_UPLOAD, total_bytes - i)
            count_needed = size // 4
            gpu.conn.send_command(f"dump {curr_addr} {count_needed}", delay=0.05)
            
            curr_chunk_res = []
            read_start = time.time()
            while len(curr_chunk_res) < count_needed and (time.time() - read_start < 2.0):
                lines = gpu.conn.read_lines()
                for line in lines:
                    match = re.match(r'^([0-9a-fA-F]{4,8}):\s+(\d+)$', line.strip())
                    if match:
                        val = int(match.group(2))
                        if val > 2147483647: val -= 4294967296
                        val = abs(val)
                        val = max(0, min(255, val))
                        curr_chunk_res.append(val)
                        if len(curr_chunk_res) >= count_needed:
                            break
                if len(curr_chunk_res) >= count_needed:
                    break
                        
            while len(curr_chunk_res) < count_needed:
                curr_chunk_res.append(0)
            
            chunk_result.extend(curr_chunk_res)
            downloaded += size
            print_progress(downloaded, total_bytes, prefix='Downl: ', length=30)
            
        print("    âœ… Complete")

        chunk_img = np.array(chunk_result).reshape((CHUNK_ROWS, w))
        output_array[row_start:row_end, :] = chunk_img

    total_time = time.time() - start_time
    print(f"\nâ±ï¸  Total Processing Time: {total_time:.2f}s (FPS: {1/total_time:.2f})")
    
    gpu.free_all()
    return img_array, output_array


def pytorch_edge_detection(image: np.ndarray) -> np.ndarray:
    """
    ä½¿ç”¨ PyTorch å¯¦ç¾ç›¸åŒçš„é‚Šç·£æª¢æ¸¬ç®—æ³•ï¼ˆä½œç‚º Ground Truthï¼‰
    
    ç®—æ³•ï¼šæ°´å¹³æ¢¯åº¦ gradient = ABS(current - previous) * 3
    """
    # è½‰æ›ç‚º tensor
    img_tensor = torch.from_numpy(image.astype(np.float32))
    
    result = np.zeros_like(image, dtype=np.float32)
    
    # é€è¡Œè™•ç†ï¼ˆæ¨¡æ“¬ ESP32 çš„ tile-based åŸ·è¡Œï¼‰
    for row_idx in range(img_tensor.shape[0]):
        row = img_tensor[row_idx, :]
        
        # è¨ˆç®—æ¢¯åº¦ (ä½¿ç”¨çµ•å°å€¼)
        gradients = torch.zeros_like(row)
        for i in range(1, len(row)):
            # ä¿®æ­£ï¼šä½¿ç”¨ abs() å–çµ•å°å€¼
            gradient = abs(row[i] - row[i-1]) * 3
            gradients[i] = gradient
        
        # Clamp åˆ° 0-255
        gradients = torch.clamp(gradients, 0, 255)
        result[row_idx, :] = gradients.numpy()
    
    return result.astype(np.int32)


def  visualize_results(original: np.ndarray, esp32_result: np.ndarray, torch_result: np.ndarray) -> None:
    """ä½¿ç”¨ Matplotlib å¯è¦–åŒ–ä¸‰å¼µåœ–ç‰‡ä¸¦è¨ˆç®—èª¤å·®æŒ‡æ¨™"""
    
    # è¨ˆç®—èª¤å·®æŒ‡æ¨™
    mse = np.mean((esp32_result - torch_result) ** 2)
    mae = np.mean(np.abs(esp32_result - torch_result))
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    axes[0].imshow(original, cmap='gray', vmin=0, vmax=255)
    axes[0].set_title('Original Image (Host)', fontsize=14, fontweight='bold')
    axes[0].axis('off')
    
    axes[1].imshow(esp32_result, cmap='gray', vmin=0, vmax=255)
    axes[1].set_title(f'ESP32 MicroGPU Result\n(VRAM-based)', fontsize=14, fontweight='bold')
    axes[1].axis('off')
    
    axes[2].imshow(torch_result, cmap='gray', vmin=0, vmax=255)
    axes[2].set_title(f'PyTorch Reference\n(Ground Truth)', fontsize=14, fontweight='bold')
    axes[2].axis('off')
    
    plt.suptitle(f'ESP32 MicroGPU Edge Detection Demo\nMSE: {mse:.2f} | MAE: {mae:.2f}', 
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('microgpu_result.png', dpi=150, bbox_inches='tight')
    print(f"\nğŸ’¾ Saved visualization: microgpu_result.png")
    print(f"ğŸ“Š Error Metrics:")
    print(f"   - MSE (Mean Squared Error): {mse:.2f}")
    print(f"   - MAE (Mean Absolute Error): {mae:.2f}")
    # plt.show()


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "ğŸ® " * 35)
    print("ESP32 MicroGPU - Image Processing Demo")
    print("ğŸ® " * 35)
    
    # åˆå§‹åŒ– GPU
    gpu = MicroGPU()
    
    # è™•ç†åœ–åƒ (ä½¿ç”¨çœŸå¯¦åœ–ç‰‡ä¸¦ç¸®æ”¾)
    image_path = '/Users/hungwei/Downloads/IMG_8152.JPG'
    print(f"\nğŸ¨ Using image: {image_path}")
    
    if not os.path.exists(image_path):
        print(f"âŒ Error: Image not found at {image_path}")
        return

    original, esp32_processed = process_image_with_microgpu(
        image_path,
        gpu,
        tile_size=128 # Actual size handling is internal
    )
    
    # ä½¿ç”¨ PyTorch è¨ˆç®—åƒè€ƒçµæœ
    print("\n" + "=" * 70)
    print("ğŸ”¥ PyTorch Reference Calculation")
    print("=" * 70)
    torch_processed = pytorch_edge_detection(original)
    print(f"âœ… PyTorch edge detection complete")
    
    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 70)
    print("ğŸ“Š Results Comparison")
    print("=" * 70)
    print(f"\nOriginal (sample):")
    print(original[:3, :3])
    print(f"\nESP32 Result (sample):")
    print(esp32_processed[:3, :3])
    print(f"\nPyTorch Result (sample):")
    print(torch_processed[:3, :3])
    
    # å¯è¦–åŒ–ï¼ˆ3å¼µåœ–ï¼‰
    visualize_results(original, esp32_processed, torch_processed)
    
    print("\n" + "=" * 70)
    print("âœ… Demo Complete!")
    print("=" * 70)


if __name__ == "__main__":
    main()
